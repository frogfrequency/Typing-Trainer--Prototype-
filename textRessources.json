{
    "bakterien": "Forscher teilen die unterschiedlichen Lebensformen auf unserem Planeten systematisch ein: Welche Organismen haben die gleichen Eigenschaften? Welche Bausteine enthalten Zellen und wie verhalten sie sich im Stoffwechsel? Wer passt zu wem? Wie hat die Erde die Organismen verändert und wie haben die Organismen die Lebensbedingungen der Erde verändert? Bakterien bilden die einfachste Lebensform auf unserem Planeten. Zwischen ihnen und anderen Zellen erfolgt eine grundlegende Unterscheidung. Bakterien fehlt im Gegensatz zu Organismen wie Alge, Pilz, Pflanze, Tier und Mensch ein Zellkern. Wissenschaftler bezeichnen sie als \"Prokaryonten\" – Zellen ohne Kern. Ihnen gegenüber stehen die \"Eukaryonten\", die alle anderen Zellen umfassen. Bakterien sind Einzeller. Zwar leben einige in Haufen zusammen, doch sind dies keine echten Verbände, die einen Austausch von Substanzen pflegen würden. Meist hängen sie rein physisch aneinander, weil sich ihre Wände nach der Teilung nicht richtig abgeschnürt haben. Zwei Vorschläge für Stammbäume, die zeigen, dass das Leben aus einer Art Urzelle entstanden sein muss. Von dem Hauptast, der von der Urzelle abgeht, zweigen Archaeen, Bakterien und höhere Zellen ab. Wie genau der Stammbaum des Lebens aussieht, ist unklar. Die Erbinformation der Bakterienzelle liegt frei im Zellsaft – lediglich in einer kernähnlichen Region ringförmig angeordnet. Mitochondrien und Chloroplasten fehlen ihnen. Beides sind Organellen, die von Doppelmembranen eingeschlossen sind und entscheidende Stoffwechselprozesse in der Zelle übernehmen. Mitochondrien sind die Kraftwerke der Zelle, weil sie mit ihrer Zellatmung die Energiegewinnung sicherstellen. Sie finden sich in den Zellen von Tieren und Pflanzen. Chloroplasten dienen der Photosynthese und enthalten den grünen Pflanzenstoff, der vor allem die Blätter färbt. Anstelle der Organellen haben Bakterien einfache Eiweißstrukturen, die die Funktionen der Energiegewinnung auf ähnliche Weise übernommen haben. Mit der Entdeckung der Archaebakterien um 1980 erfolgte eine innere Spaltung der Bakterien in Archaeen und Bakterien (auch Archaebakterien und Eu-Bakterien). Untersuchungen haben gezeigt, dass die beiden Gruppen viel zu unterschiedlich sind. Sie müssen sich in einer sehr frühen Phase des Lebens unabhängig voneinander entwickelt haben. Beide werden heute nur noch als Prokaryonten zusammengefasst. Bazillen, Kokken und Spirillen: Die drei Hauptformen der klassischen Bakterien sind die stäbchenförmigen Bazillen, rundliche Kokken und gebogene bis geschraubte Spirillen. Zu den Stäbchen gehören unser Darmbakterium Escherichia Coli, die Erreger des Wundstarrkrampfes, der Diphterie, der Tuberkulose und zahlreiche Pflanzenkrankheiten wie Feuerbrand an Birne und Apfel sowie Schleimfäule von Tomate und Banane. Kokken umfassen etwa den Keim der Lungenentzündung und Hirnhautentzündung. Spirillen sind selten. Entlarvt werden Bakterien über unterschiedliche Färbelösungen und Färbemethoden. Zur ersten groben Einteilung gehört die sogenannte Gram-Färbung, die die Zellwände violett erscheinen lässt. Es gibt Bakterien, die Sauerstoffatmung betreiben; solche, die sowohl ohne als auch mit Sauerstoff existieren können und solche, die Sauerstoff nicht tolerieren. Die Vermehrung erfolgt über Zellteilung.",
    "maxTegmark": "Although a pocket calculator can crush me in an arithmetic contest, it will never improve its speed or accuracy, no matter how much it practices. It doesn’t learn: for example, every time I press its square-root button, it computes exactly the same function in exactly the same way. Similarly, the first computer program that ever beat me at chess never learned from its mistakes, but merely implemented a function that its clever programmer had designed to compute a good next move. In contrast, when Magnus Carlsen lost his first game of chess at age five, he began a learning process that made him the World Chess Champion eighteen years later. The ability to learn is arguably the most fascinating aspect of general intelligence. We’ve already seen how a seemingly dumb clump of matter can remember and compute, but how can it learn? We’ve seen that finding the answer to a difficult question corresponds to computing a function, and that appropriately arranged matter can calculate any computable function. When we humans first created pocket calculators and chess programs, we did the arranging. For matter to learn, it must instead rearrange itself to get better and better at computing the desired function—simply by obeying the laws of physics. To demystify the learning process, let’s first consider how a very simple physical system can learn the digits of π and other numbers. Above we saw how a surface with many valleys (see figure 2.3) can be used as a memory device: for example, if the bottom of one of the valleys is at position x = π ≈ 3.14159 and there are no other valleys nearby, then you can put a ball at x = 3 and watch the system compute the missing decimals by letting the ball roll down to the bottom. Now, suppose that the surface is made of soft clay and starts out completely flat, as a blank slate. If some math enthusiasts repeatedly place the ball at the locations of each of their favorite numbers, then gravity will gradually create valleys at these locations, after which the clay surface can be used to recall these stored memories. In other words, the clay surface has learned to compute digits of numbers such as π. Other physical systems, such as brains, can learn much more efficiently based on the same idea. John Hopfield showed that his above-mentioned network of interconnected neurons can learn in an analogous way: if you repeatedly put it into certain states, it will gradually learn these states and return to them from any nearby state. If you’ve seen each of your family members many times, then memories of what they look like can be triggered by anything related to them. Neural networks have now transformed both biological and artificial intelligence, and have recently started dominating the AI subfield known as machine learning (the study of algorithms that improve through experience). Before delving deeper into how such networks can learn, let’s first understand how they can compute. A neural network is simply a group of interconnected neurons that are able to influence each other’s behavior. Your brain contains about as many neurons as there are stars in our Galaxy: in the ballpark of a hundred billion. On average, each of these neurons is connected to about a thousand others via junctions called synapses, and it’s the strengths of these roughly hundred trillion synapse connections that encode most of the information in your brain. We can schematically draw a neural network as a collection of dots representing neurons connected by lines representing synapses (see figure 2.9). Real-world neurons are very complicated electrochemical devices looking nothing like this schematic illustration: they involve different parts with names such as axons and dendrites, there are many different kinds of neurons that operate in a wide variety of ways, and the exact details of how and when electrical activity in one neuron affects other neurons is still the subject of active study. However, AI researchers have shown that neural networks can still attain human-level performance on many remarkably complex tasks even if one ignores all these complexities and replaces real biological neurons with extremely simple simulated ones that are all identical and obey very simple rules. The currently most popular model for such an artificial neural network represents the state of each neuron by a single number and the strength of each synapse by a single number. In this model, each neuron updates its state at regular time steps by simply averaging together the inputs from all connected neurons, weighting them by the synaptic strengths, optionally adding a constant, and then applying what’s called an activation function to the result to compute its next state. *5 The easiest way to use a neural network as a function is to make it feedforward, with information flowing only in one direction, as in figure 2.9, plugging the input to the function into a layer of neurons at the top and extracting the output from a layer of neurons at the bottom.",
    "numbers4567": "4567564756 5456 7564 5754 475774 5564 447 5574 475 47656 574655654755 56556475656475655 56547 56575 4445 5757 5775655"
}